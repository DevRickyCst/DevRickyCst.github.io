{
  "title": "Experiences",
  "experiences": [
    {
      "id": "0",
      "title": "Data Scientist",
      "compagny": "National Chiao Tung University",
      "compagny_categ": "Internship",
      "linkedin_url": "https://www.linkedin.com/school/national-chiao-tung-university/",
      "from": "June 2018",
      "to": "September 2018",
      "details": {
        "Subject: Detecting the Exercise Being Performed at the Gym from Data Recorded by Position Sensors on an Individual.": [
          "Analysis and understanding of coordinate data (x, y, z) over time.",
          "Utilization of Dynamic Time Warping (DTW) applied to each axis as a comparison criterion.",
          "Processing in python to implement an algorithm based on the different values found with the DTW method (pandas, numpy, dtw-python, scikit-learn)."
        ]
      },
      "stack": {
        "green": ["Python", "DTW (Dynamic Time Warping)"],
        "teal": ["Pandas", "Numpy", "Scikit-learn"],
        "grey": ["Data Exploration", "Data Analysis", "Machine Learning"]
      }
    },
    {
      "id": "1",
      "title": "FullStack Developer / Data Analyst",
      "compagny": "T&E Vision",
      "compagny_categ": "Internship",
      "from": "April 2019",
      "to": "Aug 2019",
      "details": {
        "Creation of a web reporting MVP for data analysis in the Travel & Expense sector": [
          "Data exploration and processing with python (pandas, numpy).",
          "Creating a Web interface with an authentication system (HTML, JS, PHP).",
          "Allocation of a dedicated virtual machine connected to a DNS to bring the interface online."
        ]
      },
      "stack": {
        "green": ["Python", "HTML/JS", "PHP", "Tableau Desktop"],
        "teal": ["Pandas", "Numpy"],
        "grey": ["Data Analysis", "Web Development", "Deployment"]
      }
    },
    {
      "id": "2",
      "title": "FullStack Developer",
      "compagny": "Canton-Consulting",
      "compagny_categ": "FinTech",
      "linkedin_url": "https://www.linkedin.com/company/canton-consulting/",
      "from": "Sept 2019",
      "to": "Sept 2020",
      "details": {
        "Maintenance and updating of a web application for reporting and analyzing bank charges": [
          "Development of new connection information and account management features with an LDAP authentication system (Python dash, flask, python-ldap).",
          "Maintenance of CI/CD on our internal servers with GitHub CI/CD"
        ],
        "Installation and integration of an LDAP dedicated to various internal projects.": [
          "Understanding LDAP concepts, installation and security for different needs.",
          "Integration of LDAP as an authentication system for various projects (Python, JS)."
        ],
        "DPO (Data Protection Officer)": [
          "Verification that the use of internal project data complies with the GDPR."
        ],
        "Cleaning of a database for a client needs using python (regex, pandas, numpy)": []
      },
      "stack": {
        "green": ["LDAP", "Python"],
        "teal": ["Flask", "Dash", "Sqlalchemy", "Pandas", "Numpy"],
        "grey": ["FullStack development", "Testing", "CI/CD"]
      }
    },
    {
      "id": "3",
      "title": "Data Engineer / Back-End Developer",
      "compagny": "Mobsuccess",
      "compagny_categ": "Adtech",
      "linkedin_url": "https://www.linkedin.com/company/mobsuccess/",
      "from": "April 2021",
      "to": "Present",
      "details": {
        "Design and Deployment of an Airflow MWAA Infrastructure on AWS": [
          "Deployment and maintenance of an MWAA instance (AWS Managed Apache Airflow) with CloudFormation (IaC).",
          "Use of Airflow as the central orchestrator for data workflows (DBT, PySpark, Python scripts, etc.)",
          "Automation of DAG creation using a DAG factory logic.",
          "Implementation of a development architecture and cursor rules to facilitate the development of new AI-powered pipelines."
        ],
        "Refactoring Batch Pipelines into an Event-Driven ACID Architecture with Airflow 3 + Iceberg": [
          "Replacement of CRON scheduling with Airflow 3 `Dataset` event triggers",
          "Migration of Athena tables to Iceberg (cataloged in AWS Glue) to enable ACID data updates",
          "Adaptation of dbt models to a fine-grained incremental mode based on `updated_at`",
          "Backend â†’ Airflow integration via API call to notify the arrival of new data"
        ],
        "Development and Automation of Data Ingestion Pipelines with PyAirbyte": [
          "Implementation of Airbyte connectors via the PyAirbyte SDK to extract and transform multi-source data",
          "Scheduling and orchestration of Airbyte ingestion jobs on AWS EMR clusters via Apache Airflow",
          "Design of a secure AWS infrastructure with CloudFormation, including S3 bucket management, IAM users, Secrets Manager, and Glue permissions for Airbyte",
          "Implementation of an Aurora Postgres database used as a cache for Airbyte to optimize incremental syncs and avoid data duplication"
        ],
        "ETL & Data Processing": [
          "Data extraction from various APIs (Facebook Ads, Google Ads, Snapchat Marketing...)",
          "Development of data pipelines with PySpark on AWS EMR for large-scale data processing",
          "Migration of pipelines to Apache Iceberg tables via pyIceberg, PySpark & Athena",
          "Implementation of Airflow scripts to organize and manage dbt pipeline incrementality"
        ]
      },
      "stack": {
        "green": ["AWS (Amazon Web Service)", "Python"],
        "teal": ["Airflow", "AWS Chalice", "Pyspark"],
        "grey": [
          "Microservice",
          "DevOps",
          "Data engineering",
          "API REST",
          "ETL"
        ]
      }
    }
  ]
}

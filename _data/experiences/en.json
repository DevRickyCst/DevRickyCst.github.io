{
  "title": "Experiences",
  "experiences": [
    {
      "id": "0",
      "title": "Data Scientist",
      "compagny": "National Chiao Tung University",
      "compagny_categ": "Internship",
      "linkedin_url": "https://www.linkedin.com/school/national-chiao-tung-university/",
      "from": "June 2018",
      "to": "September 2018",
      "details": {
        "Subject: Detecting the Exercise Being Performed at the Gym from Data Recorded by Position Sensors on an Individual.": [
          "Analysis and understanding of coordinate data (x, y, z) over time.",
          "Utilization of Dynamic Time Warping (DTW) applied to each axis as a comparison criterion.",
          "Processing in python to implement an algorithm based on the different values found with the DTW method (pandas, numpy, dtw-python, scikit-learn)."
        ]
      },
      "stack": {
        "green": ["Python", "DTW (Dynamic Time Warping)"],
        "teal": ["Pandas", "Numpy", "Scikit-learn"],
        "grey": ["Data Exploration", "Data Analysis", "Machine Learning"]
      }
    },
    {
      "id": "1",
      "title": "FullStack Developer / Data Analyst",
      "compagny": "T&E Vision",
      "compagny_categ": "Internship",
      "from": "April 2019",
      "to": "Aug 2019",
      "details": {
        "Creation of a web reporting MVP for data analysis in the Travel & Expense sector": [
          "Data exploration and processing with python (pandas, numpy).",
          "Creating a Web interface with an authentication system (HTML, JS, PHP).",
          "Allocation of a dedicated virtual machine connected to a DNS to bring the interface online."
        ]
      },
      "stack": {
        "green": ["Python", "HTML/JS", "PHP", "Tableau Desktop"],
        "teal": ["Pandas", "Numpy"],
        "grey": ["Data Analysis", "Web Development", "Deployment"]
      }
    },
    {
      "id": "2",
      "title": "FullStack Developer",
      "compagny": "Canton-Consulting",
      "compagny_categ": "FinTech",
      "linkedin_url": "https://www.linkedin.com/company/canton-consulting/",
      "from": "Sept 2019",
      "to": "Sept 2020",
      "details": {
        "Maintenance and updating of a web application for reporting and analyzing bank charges": [
          "Development of new connection information and account management features with an LDAP authentication system (Python dash, flask, python-ldap).",
          "Maintenance of CI/CD on our internal servers with GitHub CI/CD"
        ],
        "Installation and integration of an LDAP dedicated to various internal projects.": [
          "Understanding LDAP concepts, installation and security for different needs.",
          "Integration of LDAP as an authentication system for various projects (Python, JS)."
        ],
        "DPO (Data Protection Officer)": [
          "Verification that the use of internal project data complies with the GDPR."
        ],
        "Cleaning of a database for a client needs using python (regex, pandas, numpy)": []
      },
      "stack": {
        "green": ["LDAP", "Python"],
        "teal": ["Flask", "Dash", "Sqlalchemy", "Pandas", "Numpy"],
        "grey": ["FullStack development", "Testing", "CI/CD"]
      }
    },
    {
      "id": "3",
      "title": "Data Engineer / Back-End Developer",
      "compagny": "Mobsuccess",
      "compagny_categ": "Adtech",
      "linkedin_url": "https://www.linkedin.com/company/canton-consulting/",
      "from": "April 2021",
      "to": "Present",
      "details": {
        "Migration from batch pipelines to an event-driven ACID architecture with Airflow 3 + Iceberg": [
          "Replaced CRON-based scheduling with event-driven execution using Airflow 3 `Dataset` triggers",
          "Adopted the Iceberg table format to enable ACID-compliant data handling on S3 (via Glue Catalog)",
          "Refactored dbt models to use fine-grained incremental logic based on `updated_at` timestamps",
          "Integrated backend with Airflow via API calls to trigger DAGs upon new data arrival"
        ],
        "Development and automation of data ingestion pipelines using PyAirbyte on AWS EMR": [
          "Implemented Airbyte connectors via the PyAirbyte SDK to extract and transform data from multiple sources",
          "Scheduled and orchestrated Airbyte ingestion jobs on AWS EMR clusters using Apache Airflow",
          "Designed a secure AWS infrastructure with CloudFormation, including S3 buckets, IAM users, Secrets Manager, and Glue permissions for Airbyte",
          "Utilized Airbyte Postgres cache to optimize incremental syncs and prevent data duplication"
        ],
        "Setup, development, and maintenance of an MWAA (Airflow) instance on AWS": [
          "Development of an Airflow stack with AWS CloudFormation and SAM.",
          "Creation of bash scripts and CI/CD to automate the deployment of the stack.",
          "Implementation of DAGs to orchestrate various data jobs (DBT, EMR).",
          "Development of scripts to import internal data from various APIs (Facebook Ads, Google Ads, etc.)."
        ]
      },
      "stack": {
        "green": ["AWS (Amazon Web Service)", "Python"],
        "teal": ["Airflow", "AWS Chalice", "Pyspark"],
        "grey": [
          "Microservice",
          "DevOps",
          "Data engineering",
          "API REST",
          "ETL"
        ]
      }
    }
  ]
}

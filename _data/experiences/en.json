{
  "experiences": [
    {
      "id": "0",
      "title": "Data Scientist",
      "compagny": "National Chiao Tung University",
      "compagny_categ": "Internship",
      "linkedin_url": "https://www.linkedin.com/school/national-chiao-tung-university/",
      "from": "June 2018",
      "to": "September 2018",
      "details": {
        "Subject: Detecting the Exercise Being Performed at the Gym from Data Recorded by Position Sensors on an Individual.": [
          "Analysis and understanding of coordinate data (x, y, z) over time.",
          "Utilization of Dynamic Time Warping (DTW) applied to each axis as a comparison criterion.",
          "Processing in python to implement an algorithm based on the different values found with the DTW method (pandas, numpy, dtw-python, scikit-learn)."
        ]
      },
      "stack": {
        "green": [
          "Python",
          "Machine Learning",
          "DTW (Dynamic Time Warping)"
        ],
        "teal": [
          "Pandas",
          "Numpy"
        ],
        "grey": [
          "Data Exploration",
          "Data Analysis"
        ]
      }
    },
    {
      "id": "1",
      "title": "FullStack Developer / Data Analyst",
      "compagny": "T&E Vision",
      "compagny_categ": "Internship",
      "from": "April 2019",
      "to": "Aug 2019",
      "details": {
        "Creation of a web reporting MVP for data analysis in the Travel & Expense sector": [
          "Data exploration and processing with python (pandas, numpy).",
          "Creating a Web interface with an authentication system (HTML, JS, PHP).",
          "Allocation of a dedicated virtual machine connected to a DNS to bring the interface online."
        ]
      },
      "stack": {
        "green": [
          "Python",
          "HTML/JS",
          "PHP",
          "Tableau Desktop"
        ],
        "teal": [
          "Pandas",
          "Numpy"
        ],
        "grey": [
          "Data Analysis",
          "Web Development",
          "Deployment"
        ]
      }
    },
    {
      "id": "2",
      "title": "FullStack Developer",
      "compagny": "Canton-Consulting",
      "compagny_categ": "FinTech",
      "linkedin_url": "https://www.linkedin.com/company/canton-consulting/",
      "from": "Sept 2019",
      "to": "Sept 2020",
      "details": {
        "Maintenance and updating of a web application for reporting and analyzing bank charges": [
          "Development of new connection information and account management features with an LDAP authentication system (Python dash, flask, python-ldap).",
          "Maintenance of CI/CD on our internal servers with GitHub CI/CD"
        ],
        "Installation and integration of an LDAP dedicated to various internal projects.": [
          "Understanding LDAP concepts, installation and security for different needs.",
          "Integration of LDAP as an authentication system for various projects (Python, JS)."
        ],
        "DPO (Data Protection Officer)": [
          "Verification that the use of internal project data complies with the GDPR."
        ],
        "Cleaning of a database for a client needs using python (regex, pandas, numpy)": []
      },
      "stack": {
        "green": [
          "Python",
          "Linux",
          "LDAP"
        ],
        "teal": [
          "Flask",
          "Dash",
          "Sqlalchemy",
          "Pandas",
          "Numpy"
        ],
        "grey": [
          "Web development",
          "Testing",
          "CI/CD"
        ]
      }
    },
    {
      "id": "3",
      "title": "Data Engineer / Back-End Developer",
      "compagny": "Mobsuccess",
      "compagny_categ": "Adtech",
      "linkedin_url": "https://www.linkedin.com/company/canton-consulting/",
      "from": "April 2021",
      "to": "Present",
      "details": {
        "Implementation of data pipelines to extract, process, and import large amounts of data from various sources on a daily basis.": [
          "Creation of ETL using AWS Athena queries and Python scripts triggered with MWAA to create a data warehouse.",
          "Database management with AWS S3 using partitions to make each Airflow job idempotent and optimized for querying.",
          "Python script optimization to reduce meta datalake import time by a factor of 4.",
          "Development of Python operator on Aiflow to import daily data from advertising campaigns on Waze."
        ],
        "Deployement of python application in order to make easy access for internal data.": [
          "Development and deployment of a REST API on AWS Lambda using Chalice to retrieve data for a front-end web dashboard."
        ],
        "Development of an internal Python package to interact with various advertising platform APIs.": [
          "Development of a Python interface to retrieve data directly from the Facebook Ads and Google Ads platforms for different granularities and KPIs.",
          "Adherence to Python package standards and dependency management..",
          "CI/CD maintenance to automatically deploy this package on AWS CodeArtifact."
        ]
      },
      "stack": {
        "green": [
          "AWS (Amazon Web Service)",
          "Python"
        ],
        "teal": [
          "Airflow",
          "AWS Chalice",
          "Pyspark"
        ],
        "grey": [
          "Microservice",
          "DevOps",
          "Data engineering",
          "API REST",
          "Packaging"
        ]
      }
    }
  ]
}